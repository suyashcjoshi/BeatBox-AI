{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Images from Audio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FoundArguments, will start converting\n",
      "Starting on -f\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../SpectrumVarialbes.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a32879ca7560>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mNameError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Type' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a32879ca7560>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mlog_mel_spec_tfm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mType\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"INTERFACE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-a32879ca7560>\u001b[0m in \u001b[0;36mlog_mel_spec_tfm\u001b[0;34m(dataInput)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m#print(src_path, dst_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Starting on'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mpictures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerateSpectrums\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpictures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-a32879ca7560>\u001b[0m in \u001b[0;36mGenerateSpectrums\u001b[0;34m(MainFile)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mGenerateSpectrums\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMainFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mSpectrumVariables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../SpectrumVarialbes.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../SpectrumVarialbes.csv'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import librosa\n",
    "import wave\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import pickle\n",
    "\n",
    "from ipywidgets import interactive\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from PIL import Image\n",
    "import IPython.display as displayImg\n",
    "\n",
    "from ipywidgets import interact, widgets\n",
    "import glob\n",
    "import IPython.display as ipd\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "import torchvision\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import sys\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "def listdir_nohidden(path):\n",
    "    return glob.glob(os.path.join(path, '*'))\n",
    "\n",
    "def GenerateSpectrums(MainFile):\n",
    "    SpectrumVariables={}\n",
    "    with open('SpectrumVarialbes.csv', newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            for k in row:\n",
    "                SpectrumVariables[k]=int(row[k])\n",
    "\n",
    "    x ,sample_rate_in = librosa.load(MainFile,mono=True)\n",
    "    audio_data = librosa.resample(x, sample_rate_in, SpectrumVariables['SAMPLE_RATE'])\n",
    "    mel_spec_power = librosa.feature.melspectrogram(audio_data, sr=SpectrumVariables['SAMPLE_RATE'],\n",
    "                                                    n_fft=SpectrumVariables['N_FFT'],\n",
    "                                                    hop_length=SpectrumVariables['HOP_LENGTH'],\n",
    "                                                    n_mels=SpectrumVariables['N_MELS'],\n",
    "                                                    power=SpectrumVariables['POWER'],\n",
    "                                                   fmin=SpectrumVariables['FMIN'],\n",
    "                                                    fmax=SpectrumVariables['FMAX'])\n",
    "    mel_spec_db = np.float32(librosa.power_to_db(mel_spec_power, ref=np.max))\n",
    "    mel_spec_db-=mel_spec_db.min()\n",
    "    mel_spec_db/=mel_spec_db.max()\n",
    "    im = np.uint8(cm.gist_earth(mel_spec_db)*255)[:,:,:3]\n",
    "    ArrayofPictures = []\n",
    "    RESOLUTION = SpectrumVariables['RESOLUTION']\n",
    "    for i in range(int(np.floor(im.shape[1]/RESOLUTION))):\n",
    "        startx=RESOLUTION*i\n",
    "        stopx=RESOLUTION*(i+1)\n",
    "        ArrayofPictures.append(im[:,startx:stopx,:])\n",
    "    return ArrayofPictures\n",
    "\n",
    "def log_mel_spec_tfm(dataInput):\n",
    "    src_path=dataInput[0]\n",
    "    dst_path=dataInput[1]\n",
    "    #print(src_path, dst_path)\n",
    "    print('Starting on',os.path.split(src_path)[1])\n",
    "    pictures = GenerateSpectrums(src_path)\n",
    "    print(len(pictures))\n",
    "    fname = os.path.split(src_path)[-1]\n",
    "    count=0\n",
    "    for pic in pictures:\n",
    "        plt.imsave(os.path.join(dst_path,(fname.replace(\".flac\",'-')\\\n",
    "                                          .replace(\".aif\",'-').replace(\".wav\",'-')\\\n",
    "                                          .replace(\".m4a\",'-').replace(\".mp3\",'-')\\\n",
    "                                          +str(count)+'.png')), pic)\n",
    "        count+=1\n",
    "    if(count==0):\n",
    "        print(src_path)\n",
    "\n",
    "\n",
    "try:\n",
    "    Type\n",
    "except NameError:\n",
    "    if(len(sys.argv)>1):\n",
    "        print(\"FoundArguments, will start converting\")\n",
    "        source = str(sys.argv[1])\n",
    "        target = str(sys.argv[2])\n",
    "        log_mel_spec_tfm((source,target))\n",
    "else:\n",
    "    if(Type==\"INTERFACE\"):\n",
    "        SOURCE_DATA_ROOT='../AudioData/'\n",
    "\n",
    "        style = {'description_width': 'initial'}\n",
    "\n",
    "        ClassSelection = widgets.Dropdown(options=listdir_nohidden(SOURCE_DATA_ROOT), description='Source for Training Data:',style=style)\n",
    "        FileSelection = widgets.Dropdown(description='Audio file to visualize',style=style)\n",
    "\n",
    "        def updateLocation(*args):\n",
    "            FileSelection.options=listdir_nohidden(os.path.join(SOURCE_DATA_ROOT,ClassSelection.value))\n",
    "\n",
    "        ClassSelection.observe(updateLocation)\n",
    "\n",
    "        display(ClassSelection)\n",
    "        display(FileSelection)\n",
    "        updateLocation();\n",
    "    elif(Type==\"TRAINING\"):\n",
    "        SPECTRUM_IMAGES_ROOT=\"../GeneratedData/\"\n",
    "        class SpectrumDataset(torch.utils.data.Dataset):\n",
    "            \"\"\"Face Landmarks dataset.\"\"\"\n",
    "            def __init__(self,ClassName,root_dir,transform=None):\n",
    "                \"\"\"\n",
    "                Args:\n",
    "                    root_dir (string): Directory with all the images.\n",
    "                    transform (callable, optional): Optional transform to be applied\n",
    "                        on a sample.\n",
    "                \"\"\"\n",
    "                self.root_dir = root_dir\n",
    "                self.ClassName=ClassName\n",
    "                self.fileList= [f for f in os.listdir(root_dir) if f.endswith('.png')]\n",
    "                print(root_dir,len(self.fileList))\n",
    "                self.transform = transform\n",
    "            def ReduceSize(self,ItemCount):\n",
    "                self.fileList = random.choices(self.fileList, k=ItemCount)\n",
    "            def __len__(self):\n",
    "                return len(self.fileList)\n",
    "            def __getitem__(self, idx):\n",
    "                if torch.is_tensor(idx):\n",
    "                    idx = idx.tolist()\n",
    "                img_path = os.path.join(self.root_dir,\n",
    "                                        self.fileList[idx])\n",
    "                image = Image.open(img_path)\n",
    "                image=image.convert('RGB')\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                return image,self.ClassName\n",
    "        classes = [os.path.split(c)[1] for c in listdir_nohidden(SPECTRUM_IMAGES_ROOT)]\n",
    "        widgetDict={}\n",
    "        print(\"Select classes to use for training:\");\n",
    "        for c in classes:\n",
    "            widgetDict[c]=widgets.Checkbox(\n",
    "            value=False,\n",
    "            description=c,\n",
    "            disabled=False,\n",
    "            indent=False)\n",
    "            display(widgetDict[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_DATA_ROOT='../AudioData/BeatBox' \n",
    "GENERATED_DATA_ROOT='../GeneratedData/BeatBox'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset - 80% of the audio files into a training folder, while 20% into testing folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../AudioData/BeatBox'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e23c588b89fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mToDoList\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mSourceFoldersLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSOURCE_DATA_ROOT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSourceFoldersLabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mFileList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".aif\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".flac\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".wav\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".m4a\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".mp3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mLabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../AudioData/BeatBox'"
     ]
    }
   ],
   "source": [
    "ToDoList=[]\n",
    "SourceFoldersLabels = [f.path for f in os.scandir(SOURCE_DATA_ROOT) if f.is_dir()]\n",
    "for path in SourceFoldersLabels:\n",
    "    FileList = np.array([f.path for f in os.scandir(path) if f.is_file() and (f.name.endswith(\".aif\") or f.name.endswith(\".flac\") or f.name.endswith(\".wav\") or f.name.endswith(\".m4a\") or f.name.endswith(\".mp3\"))])\n",
    "    Label = os.path.split(path)[-1]\n",
    "    OutFolderTrain = os.path.join(GENERATED_DATA_ROOT,Label,'train')\n",
    "    OutFolderTest = os.path.join(GENERATED_DATA_ROOT,Label,'test')\n",
    "    if not os.path.exists(OutFolderTrain):\n",
    "        os.makedirs(OutFolderTrain)\n",
    "    if not os.path.exists(OutFolderTest):\n",
    "        os.makedirs(OutFolderTest)\n",
    "    np.random.shuffle(FileList)\n",
    "    trainCount =np.int(np.floor(0.8*FileList.shape[0]))\n",
    "    train_set = FileList[:trainCount]\n",
    "    test_set = FileList[trainCount:]\n",
    "    for f in train_set:\n",
    "        ToDoList.append((os.path.abspath(f),os.path.abspath(OutFolderTrain)))\n",
    "    for f in test_set:\n",
    "        ToDoList.append((os.path.abspath(f),os.path.abspath(OutFolderTest)))\n",
    "    print(\"Finished class\",Label,\". Going to the next.\")\n",
    "Commands = [[sys.executable, \"../helperFunctions.py\",t[0],t[1]] for t in ToDoList]\n",
    "print(\"Done Creating our ToDoList. I'll start computing now, hold on.\")\n",
    "tempArray=[]\n",
    "for i in range(len(Commands)):\n",
    "    tempArray.append(Commands[i])\n",
    "    if(len(tempArray)>=12):  ## <= To optimize you can type in here how many CPU cores/threads you have \n",
    "        procs = [Popen(j) for j in tempArray ]\n",
    "        for p in procs:\n",
    "            p.wait()\n",
    "        tempArray=[]\n",
    "\n",
    "procs = [ Popen(j) for j in tempArray ]\n",
    "for p in procs:\n",
    "    p.communicate() \n",
    "print(\"All Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting the Training Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UsedClasses=[]\n",
    "for k in widgetDict:\n",
    "    if widgetDict[k].value==True:\n",
    "        UsedClasses.append(k)\n",
    "if(len(UsedClasses)<3): # kick drum, snare drum, high hat\n",
    "    print(\"Something is wrong here. we need at least 3 classes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes =  tuple(UsedClasses)\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainSets=[]\n",
    "testSets=[]\n",
    "\n",
    "for cl in classes:   \n",
    "    trainPath=os.path.join(SPECTRUM_IMAGES_ROOT,cl,'train')\n",
    "    if os.path.isdir (trainPath):\n",
    "        trainSets.append(SpectrumDataset(classes.index(cl),trainPath,transform))\n",
    "    else:\n",
    "        print('Coud not find path',trainPath);\n",
    "    \n",
    "    testPath=os.path.join(SPECTRUM_IMAGES_ROOT,cl,'test')\n",
    "    if os.path.isdir (testPath):\n",
    "        testSets.append(SpectrumDataset(classes.index(cl),testPath,transform))\n",
    "    else:\n",
    "        print('Coud not find path',testPath);\n",
    "\n",
    "\n",
    "lowestItemCount=np.inf\n",
    "classID=None\n",
    "\n",
    "for i,train in enumerate(trainSets):\n",
    "    if(lowestItemCount>len(train)):\n",
    "        lowestItemCount=len(train)\n",
    "        classID=i\n",
    "        lowestItemCount=len(train)\n",
    "for i in range(len(trainSets)):\n",
    "    trainSets[i].ReduceSize(lowestItemCount)\n",
    "    \n",
    "\n",
    "TrainDataSet = torch.utils.data.ConcatDataset(trainSets)\n",
    "TestDataSet = torch.utils.data.ConcatDataset(testSets)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(TrainDataSet, batch_size=16, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(TestDataSet, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting some random training images and showing them\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "for i in range(trainloader.batch_size):\n",
    "    imshow(images[i])\n",
    "    print(classes[labels[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.resnet18(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD PRE-TRAINED MODEL for UrbanSound dataset\n",
    "\n",
    "ModelData = torch.load('../Models/MainModelUrban.pth',map_location='cpu')\n",
    "model.load_state_dict(ModelData['model'])\n",
    "print(ModelData[\"classes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-training only the last layer of Model for Beatbox sound classficiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For each parameter in the network we turn of training, \n",
    "by setting  .requires_grad  to `False`.\n",
    "\n",
    "This makes sure that the computer will not try to adjust thos variables when \"training\".\n",
    "'''\n",
    "for param in model.parameters(): #\n",
    "    param.requires_grad = False #...\n",
    "    \n",
    "# Configuring model for training\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "train_epoch_losses=[]\n",
    "test_epoch_losses=[]\n",
    "epoch=0\n",
    "    \n",
    "'''\n",
    "'fc' stands for \"fully connected\" and it is the very last layer in the neural net.\n",
    "We replace this layer with a new fully connected layer, that connects the 512 input neurons to neurons for our classes.\n",
    "'''    \n",
    "model.fc = nn.Linear(512, len(classes)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the network on the training dataset\n",
    "\n",
    "for i in range(20):  # loop over the dataset multiple (5) times \n",
    "    epoch+=1\n",
    "    print(\"Starting epoch:\",epoch)\n",
    "    epochLoss=0.0\n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        #print(\"Running Batches\",i)\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        if device.type=='cuda':\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        if((i+i)%200==0):\n",
    "            if(i>0):\n",
    "                print('Processed images:',i*trainloader.batch_size,'. Running Timer @ {:.2f}sec.'.format(time.time()-t0))\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epochLoss+=loss.item()\n",
    "        \n",
    "    \n",
    "    model.eval()\n",
    "    testLoss=0\n",
    "    print(\"About to test the performance on the test set.\")\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            if device.type=='cuda':\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            testLoss+=loss.item()\n",
    "            if(i%50==0):\n",
    "                if(i>0):\n",
    "                    print('Tested images:',i*testloader.batch_size,'. Running Timer @ {:.2f}sec.'.format(time.time()-t0))\n",
    "\n",
    "\n",
    "    train_epoch_losses.append(epochLoss/len(trainloader))\n",
    "    test_epoch_losses.append(testLoss/len(testloader))\n",
    "    EpochLength = time.time()-t0\n",
    "    print('{} train loss: {:.3f} and test loss: {:.3f}, and it took us: {:.2f} seconds.'.format (epoch + 1, epochLoss / len(trainloader),testLoss/len(testloader),EpochLength))  # DAVID CHanged it to 1000 from 2000 not sure if thats totally done\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Trained Model and performing analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the learnd model in file that can be loaded in for inference\n",
    "torch.save({\n",
    "    'model':model.state_dict(),\n",
    "    'classes':classes,\n",
    "    'resolution':224,\n",
    "    'modelType':\"resnet18\" # <= If you try out different models make sure to change this too\n",
    "},\"../models/BeatBox.pth\") # <=Edit file name here \n",
    "\n",
    "#Displaying how the loss progresses over time.\n",
    "plt.plot(train_epoch_losses, label='Training Loss',c='r')\n",
    "plt.plot(test_epoch_losses, label='Test Loss',c='g')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print predicted and acual labels for Spectragrams just to verify trained model is working correctly\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "model.eval()\n",
    "for j in range (2):\n",
    "    images, labels = dataiter.next()\n",
    "    if device == 'cuda':\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        imshow(images[i])\n",
    "        print('GroundTruth: ',classes[labels[i]])\n",
    "        print('Predicted: ',  classes[predicted[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network analytics\n",
    "\n",
    "class_correct = list(0. for i in range(len(classes)))\n",
    "class_total = list(0. for i in range(len(classes)))\n",
    "model.eval()\n",
    "allLabels=[]\n",
    "allPrediction=[]\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        if (c.dim()==0):\n",
    "            continue\n",
    "        for i in range(testloader.batch_size):\n",
    "            if(len(labels)<=i):\n",
    "                continue;\n",
    "            label = labels[i]\n",
    "            allLabels.append(labels[i].to('cpu').numpy())\n",
    "            allPrediction.append(predicted[i].to('cpu').numpy())\n",
    "            #print (c.shape)\n",
    "            if(testloader.batch_size>1):\n",
    "\n",
    "                class_correct[label] += c[i].item()\n",
    "            else:\n",
    "                class_correct[label] += c.item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "print(confusion_matrix(allLabels, allPrediction))\n",
    "for i in range(len(classes)):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
